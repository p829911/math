# An Introduction to Statistical Learning

**다중 선형 회귀**

1. 설명변수들 $X_1, X_2, \cdots, X_p$ 중 적어도 하나는 반응변수를 예측하는데 유용한가?
2. $Y$를 설명하는 데 모든 설명변수들이 도움이 되는가? 또는 설명변수들의 일부만이 유용한가?
3. 모델은 데이터에 얼마나 잘 맞는가?
4. 주어진 설명변수 값들에 대해 어떤 반응변수 값을 예측해야 하고 그 예측은 얼마나 정확한가?



**1. 반응변수와 설명변수 사이에 상관관계가 있는가?**

단순선형회귀에서 반응변수와 설명변수 사이에 상관관계가 있는지는 단순히 $\beta_1 = 0$인지 검사하면 결정할 수 있다. $p$개 설명변수가 있는 다중회귀에서는 모든 회귀계수들이 영인지, 즉 $\beta_1 = \beta_2 = \cdots = \beta_p =0$인지를 검사해야 한다. 단순선형회귀에서와 같이 이 질문에 답하기 위해 가설검정을 사용한다.

귀무가설
$$
H_0 : \beta_1 = \beta_2 = \cdots = \beta_p = 0
$$
대립가설
$$
H_a: \text{적어도 하나의 }\beta_j\text{는 영이 아니다.}
$$
이러한 가설 검정은 $F$-통계량을 계산함으로써 이루어진다.
$$
F = \dfrac{(\text{TSS} - \text{RSS})/p}{\text{RSS}/(n-p-1)}
$$
단순선형회귀에서와 같이 $\text{TSS} = \sum(y_i-\bar{y})^2$ 이고 $\text{RSS} = \sum(y_i - \hat{y_i})^2$이다. 만약 선형 모델의 가정이 같다면 다음이 성립함을 보여줄 것이다.
$$
E[\,\text{RSS}\, / \,(n-p-1)\,] = \sigma^2
$$
또한 귀무가설 $H_0$이 참이면 다음이 성립함을 보여줄 수도 있다.
$$
E[\,(\text{TSS} - \text{RSS})\,/\,p\,] = \sigma^2
$$
그러므로, 반응변수와 설명변수들 사이에 상관관계가 없는 경우(RSS가 커질때 TSS와 거의 가까움) $F$- 통계량이 1에 매우 가까운 값이라고 기대할 수 있을 것이다. 반면에 만약 대립가설 $H_a$가 참이면 $E[\,(\text{TSS}-\text{RSS})\,/\, p\,] > \sigma^2$이고 그래서 $F​$의 기대값은 1보다 크다.

$H_0$**을 기각하고 상관관계가 있다고 결론을 내릴 수 있으면 $F$-통계량이 얼마나 커야될까?**

- $n\text{과} p$값에 따라 다르다.

- $n$이 큰 경우에는 $F$-통계량이 1보다 약간만 크면 $H_0$에 반하는 증거가 된다.
- $n$이 작은 경우 $H_0$를 기각하려면 더 큰 $F$-통계량이 필요하다.



**2. 중요 변수의 결정**

모든 설명변수가 반응변수와 상관성이 있을 수도 있다. 하지만 대부분의 경우 설명변수들의 일부(서브셋)만이 반응변수와 상관관계가 있다. 상관성이 있는 설명변수만으로 모델 적합을 수행하기 위해 어느 설명변수가 반응변수와 상관성이 있는지 결정하는 것을 변수선택이라고 한다.

모델의 질 평가

- 맬로우즈(Mallows) $C_p$
- AIC (Akaike information criterion)
- BIC (Bayesian information criterion)
- 수정된 $R^2$

$p$개 변수들의 일부를 포함하는 총 모델의 경우의 수는 $2^p$개에 이른다. 심지어 $p$가 크지 않더라도 모든 가능한 설명변수들의 부분집합을 다 시험해 보는 것은 현실적으로 어렵다. 예를 들어 $p = 2$인 경우 $2^2 = 4$모델을 고려하면 된다. 그러나 $p=30$이면 고려해야 하는 모델 수는 $2^{30} = 1,073,741,824$개로 늘어나 현실적으로 불가능에 가깝다. 그러므로 $p$가 아주 작은 경우가 아니면 $2^p$개 모델 모두를 고려할 수는 없고, 대신에 더 작은 수의 고려할 모델 집합을 선택하는 자동화되고 효과적인 기법이 필요하다. 이 목적을 위한 3가지 고전적인 기법은 아래와 같다.

- 전진선택: 이 방법은 절편만 포함하고 설명변수는 없는 영모델(null model)을 가지고 시작한다. $p$개의 단순 선형 회귀를 적합하여 가장 낮은 $RSS$가 발생되는 변수를 영모델에 추가한다. 그런 다음 새로운 새로운 2-변수 모델에 대해 가장 낮은 $RSS$가 생기는 변수를 모델에 추가한다. 이런 방식으로 어떤 정지규칙(stopping rule)을 만족할 때까지 계속된다.
- 후진선택(Backward selection): 이 방법은 모델의 모든 변수를 가지고 시작하여 가장 큰 $p$-값을 가지는 변수, 즉 통계적으로 중요도가 가장 낮은 변수를 제외한다. 그 다음에 새로운 (p-1)-변수의 모델을 적합하고 $p$-값이 가장 큰 변수를 제외한다. 이 과정을 정지 규칙이 만족될 때까지 계속한다. 예를 들어 모든 남아있는 변수들의 $p$-값이 어떤 임계치보다 작으면 이 과정을 중지한다.
- 혼합선택(Mixed selection): 이것은 전진선택과 후진선택을 결합한 것이다. 전진선택처럼 변수가 없는 모델로 시작하여 최상의 적합을 제공하는 변수를 하나씩 추가한다. 새로운 설명변수들이 모델에 추가됨에 따라 변수들에 대한 $p$-값이 커질 수 있다. 그러므로 모델의 변수들 중 어느 하나에 대한 $p$-값이 어떤 임계치보다 커지면 그 변수를 모델에서 제외한다. 이러한 전진선택 및 후진선택 단계를 계속하여 모델에 포함되는 모든 변수들은 충분히 작은 $p$-값을 가지고 모델에서 제외된 변수들은 만약 모델에 추가될 경우 $p$-값이 크게 될 때 중지한다.

후진 선택법은 만약 p > n 이면 사용할 수 없지만 전진선택법은 항상 사용할 수 있다. 전진선택법은 그리디(greedy)방식이다. 그래서 초기에 포함된 변수들이 나중에는 유효하지 않을 수 있다. 이 문제는 혼합선택법으로 선택할 수 있다.



**3. 모델 적합**

모델 적합의 수치적 측도로 가장 흔히 사용되는 두가지는 $RSE$와 $R^2$(설명되는 분산의 비율)이다. 이 값들은 단순선형회귀에서와 같은 방식으로 계산되고 해석된다.

단순회귀에서 $R^2$은 반응변수와 설명변수의 상관계수의 곱이다. 다중 선형회귀에서 이것은 반응변수와 적합된 선형모델 사이의 상관계수의 제곱인 $Cor(Y,\hat{Y})^2$과 동일하다. 사실 적합된 선형모델은 모든 가능한 선형모델 중에서 이 상관계수가 최대로 되는 것이다.

1에 가까운 $R^2$값은 모델이 반응변수 내 분산의 많은 부분을 설명한다는 것을 나타낸다.

모델에 더 많은 변수가 추가되면 비록 추가된 변수와 반응변수의 상관관계가 아주 약하더라도 $R^2$은 항상 증가할 것이다. 이것은 최소제곱 방정식에 변수를 추가하면 훈련 데이터(반드시 검정 데이터일 필요는 없다.)를 더 정확하게 적합할 수 있다는 사실 때문이다. 특정 독립 변수를 추가했을 때 $R^2​$이 약간만 증가한다는 사실은 그 독립 변수가 모델에서 제외될 수 있다는 추가적인 증거가 된다. 모델에 그 독렵변수를 포함하는 것은 독립적인 검정표본에 대한 과적합으로 인해 좋지 않은 결과를 초래할 가능성이 높을 것이다.