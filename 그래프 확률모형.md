# 그래프 확률 모형

여러 확률변수의 결합분포를 구해야 하는 경우를 생각하자. 예를 들어 A, B, C, 3개의 확률변수가 있고 각 확률변수가 0, 1, 2 세가지의 값만 가질 수 있는 카테고리 확률변수인 경우 이 세 확률변수의 결합분포는 다음과 같이 표로 나타낼 수 있다. 이 표는 $3^3 - 1 = 26​$ 개의 모수를 가지므로 (합이 1이 되어야 하므로 하나는 다른 값들에 의존한다.) 이 표를 저장하려면 26개의 저장 공간이 필요하다.

| A    | B    | C    | P(A, B, C)     |
| ---- | ---- | ---- | -------------- |
| 0    | 0    | 0    | P(A=0,B=0,C=0) |
| 0    | 0    | 1    | P(A=0,B=0,C=1) |
| 0    | 0    | 2    | P(A=0,B=0,C=2) |
| 0    | 1    | 0    | P(A=0,B=1,C=0) |
| 0    | 1    | 1    | P(A=0,B=1,C=1) |
| 0    | 1    | 2    | P(A=0,B=1,C=2) |
| 0    | 2    | 0    | P(A=0,B=2,C=0) |
| 0    | 2    | 1    | P(A=0,B=2,C=1) |
| 0    | 2    | 2    | P(A=0,B=2,C=2) |
| 1    | 0    | 0    | P(A=1,B=0,C=0) |
| 1    | 0    | 1    | P(A=1,B=0,C=1) |
| 1    | 0    | 2    | P(A=1,B=0,C=2) |
| 1    | 1    | 0    | P(A=1,B=1,C=0) |
| 1    | 1    | 1    | P(A=1,B=1,C=1) |
| 1    | 1    | 2    | P(A=1,B=1,C=2) |
| 1    | 2    | 0    | P(A=1,B=2,C=0) |
| 1    | 2    | 1    | P(A=1,B=2,C=1) |
| 1    | 2    | 2    | P(A=1,B=2,C=2) |
| 2    | 0    | 0    | P(A=2,B=0,C=0) |
| 2    | 0    | 1    | P(A=2,B=0,C=1) |
| 2    | 0    | 2    | P(A=2,B=0,C=2) |
| 2    | 1    | 0    | P(A=2,B=1,C=0) |
| 2    | 1    | 1    | P(A=2,B=1,C=1) |
| 2    | 1    | 2    | P(A=2,B=1,C=2) |
| 2    | 2    | 0    | P(A=2,B=2,C=0) |
| 2    | 2    | 1    | P(A=2,B=2,C=1) |
| 2    | 2    | 2    | P(A=2,B=2,C=2) |

### 베이지안 네트워크 모형

그런데 현실에서는 모든 확률변수가 서로 영향을 미치는 복잡한 경우 보다 특정한 몇개의 확률분포들이 서로 영향을 미친다. 예를 들어 A, B, C가 각각 어떤 학생의

- A: 건강 상태
- B: 공부 시간
- C: 시험 성적

을 나타낸 것이라고 하자. 이 확률변수는 각각 $\{0, 1, 2\}$ 라는 값을 가질 수 있는데 하(0), 중(1), 상(2)의 상태를 나타낸다. 즉 $A = 0$이면 건강상태가 안좋은 것이고 $B = 1$ 이면 공부 시간이 보통이며 $C = 2$이면 시험 성적이 좋은 것이다.

공부 시간 $B​$는 시험 성적 $C​$에 영향을 미친다. 하지만 건강 상태 $A​$는 공부 시간 $B​$와 인과 관계가 있지만 시험 성적 $C​$와는 직접적인 인과 관계가 없다. 이렇게 다수의 확률변수 중 특정한 소수의 확률변수들이 가지는 관계를 그래프로 표현한 것을 그래프 확률모형(graphical probability model)이라고 하고 그래프 확률모형 중에서도 이렇게 인과관계가 확실하여 방향성 그래프로 표시할 수 있는 것을 베이지안 네트워크 모형이라고 한다. 위의 확률변수를 베이지안 네트워크 모형으로 그리면 다음과 같다.

```python
import networkx as nx
from IPython.core.display import Image
from networkx.drawing.nx_pydot import to_pydot

g1 = nx.DiGraph()
g1.add_path(["A", "B", "C"])
d1 = to_pydot(g1)
d1.set_dpi(300)
d1.set_rankdir("LR")
d1.set_margin(0.2)
Image(d1.create_png(), width=600)
```

![](https://user-images.githubusercontent.com/17154958/51011461-f5d4f800-159b-11e9-804e-d0590b1cae0d.png)

이러한 그래프를 방향성 그래프(directed graph)라고 한다. 방향성 그래프에서 확률변수는 하나의 노드(node) 또는 정점(vertex)로 나타내고 인과관계는 화살표 간선(edge, link)으로 나타낸다. 우리가 다루는 방향성 그래프는 화살표가 여러 확률변수를 거쳐 자기 자신에게 돌아오는 루프(loop)가 없는 DAG(Directed Acyclic Graph) 모형이다.

방향성 그래프 모델에서는 원인과 결과가 되는 두 확률변수의 관계를 조건부 확률분포로 표현한다. 위의 모형에서처럼 A가 B의 원인이 된다면 이 두 확률변수의 관계를 $P(B \mid A)$ 로 나타내고 B가 C의 원인이 되므로 두 확률변수의 관계는 $P(C \mid B)$ 로 나타낸다.

그리고 전체 확률변수들간의 관계는 이러한 조건부 확률분포를 결합하여 나타낼 수 있다. 위의 그래프에서 전체 결합분포는 다음과 같다.
$$
P(A, B, C) = P(A)P(B \mid A)P(C \mid B)
$$
단, 여기에서 유의해야 할 점은 A와 B는 직접적인 인과 관계가 없지만 상관관계는 있을 수 있다는 점이다. 예를 들어 A-B, B-C간의 관계가 모두 양의 상관관계면 A가 커졌을 때 B도 커지고 따라서 C도 커지므로 A와  B가 양의 상관관계를 가지게 된다. A, B, C가 의미하는 바로 해석하면 건강 상태와 시험 성적은 직접적인 인과관계는 없지만 건강상태가 좋을 때 공부 시간도 많아질 가능성이 높고 공부 시간이 많을 때 시험 성적이 좋아진다면 건강 상태와 시험 성적은 양의 상관관계를 가질 수 있다.

결합확률분포를 이루는 팩터를 각각 표로 나타내면 다음과 같다.

| A    | 𝑃(𝐴)P(A)     |
| ---- | ------------ |
| A=0  | 𝑃(𝐴=0)P(A=0) |
| A=1  | 𝑃(𝐴=1)P(A=1) |
| A=2  | 𝑃(𝐴=2)P(A=2) |

| B    | 𝑃(𝐵\|𝐴=0)P(B\|A=0)     | 𝑃(𝐵\|𝐴=1)P(B\|A=1)     | 𝑃(𝐵\|𝐴=2)P(B\|A=2)     |
| ---- | ---------------------- | ---------------------- | ---------------------- |
| B=0  | 𝑃(𝐵=0\|𝐴=0)P(B=0\|A=0) | 𝑃(𝐵=0\|𝐴=1)P(B=0\|A=1) | 𝑃(𝐵=0\|𝐴=2)P(B=0\|A=2) |
| B=1  | 𝑃(𝐵=1\|𝐴=0)P(B=1\|A=0) | 𝑃(𝐵=1\|𝐴=1)P(B=1\|A=1) | 𝑃(𝐵=1\|𝐴=2)P(B=1\|A=2) |
| B=2  | 𝑃(𝐵=2\|𝐴=0)P(B=2\|A=0) | 𝑃(𝐵=2\|𝐴=1)P(B=2\|A=1) | 𝑃(𝐵=2\|𝐴=2)P(B=2\|A=2) |

| C    | 𝑃(𝐶\|𝐵=0)P(C\|B=0)     | 𝑃(𝐶\|𝐵=1)P(C\|B=1)     | 𝑃(𝐶\|𝐵=2)P(C\|B=2)     |
| ---- | ---------------------- | ---------------------- | ---------------------- |
| C=0  | 𝑃(𝐶=0\|𝐵=0)P(C=0\|B=0) | 𝑃(𝐶=0\|𝐵=1)P(C=0\|B=1) | 𝑃(𝐶=0\|𝐵=2)P(C=0\|B=2) |
| C=1  | 𝑃(𝐶=1\|𝐵=0)P(C=1\|B=0) | 𝑃(𝐶=1\|𝐵=1)P(C=1\|B=1) | 𝑃(𝐶=1\|𝐵=2)P(C=1\|B=2) |
| C=2  | 𝑃(𝐶=2\|𝐵=0)P(C=2\|B=0) | 𝑃(𝐶=2\|𝐵=1)P(C=2\|B=1) | 𝑃(𝐶=2\|𝐵=2)            |

이 경우에 우리가 알아야 하는 모수의 수는

- $P(A): 3- 1 = 2$
- $P(B \mid A): (3-1) \times 2 = 6$
- $P(C \mid B): (3-1) \times 2 = 6$

따라서 총 14개이다. 변수간의 인과 관계라는 추가 정보로 인해 모수의 숫자가 26개에서 14개로 크게 감소하였다. 이렇게 우리가 알고 있는 확률변수간의 정보를 그래프를 이용하여 추가하면 문제를 더 간단하게 만들 수 있다.

pgmpy 패키지를 사용하여 앞의 예제를 파이썬으로 구현해 보자. 조건부 확률 $P(A), P(B \mid A), P(C \mid B)​$ 는 `TabularCPD` 클래스로 다음과 같이 구현할 수 있다. 우선 건강 상태는 나쁠(A=0) 확률이 20%, 보통(A=1)일 확률이 60%, 좋을 확률이 20%라고 하자.

```python
from pgmpy.factors.discrete import TabularCPD

P_A = TabularCPD('A', 3, [[0.2, 0.6, 0.2]])
print(P_A)
```

![](https://user-images.githubusercontent.com/17154958/51100539-e9f66980-1819-11e9-9d13-8369f63a0168.png)

건강 상태가 나쁘면(A=0), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 50%, 30%, 20%이다. 건강 상태가 보통이면(A=1), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 20%, 60%, 20%이다. 건강 상태가 좋으면(A=2), 공부시간이 적거나(B=0), 보통이거나(B=1), 많을(B=2) 확률은 각각 20%, 30%, 50%이다.

 ```python
P_B_I_A = TabularCPD('B', 3, 
    np.array([[0.6, 0.2, 0.2], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]),
    evidence=['A'], evidence_card=[3])
print(P_B_I_A)
 ```

![](https://user-images.githubusercontent.com/17154958/51100631-4063a800-181a-11e9-87f8-f556494084e1.png)

비슷하게 공부시간이 적으면(B=0), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 80%, 10%, 10%이다. 공부시간이 보통이면(B=1), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 10%, 80%, 10%이다. 공부시간이 많으면(B=2), 성적이 나쁘거나(C=0), 보통이거나(C=1), 좋을(C=2) 확률은 각각 10%, 10%, 80%이다.

```python
P_C_I_B = TabularCPD('C', 3, 
    np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]),
    evidence=['B'], evidence_card=[3])
print(P_C_I_B)
```

![](https://user-images.githubusercontent.com/17154958/51100675-730da080-181a-11e9-971c-8d16646d1af2.png)

이 조건부 확률들을 결합하여 하나의 베이지안 네트워크로 만들려면 `BayesianModel` 클래스를 사용한다. 생성자에는 노드를 연결한 그래프 정보를 넣고 `add_cpds` 메서드로 조건부확률을 추가할 수 있다.

```python
from pgmpy.models import BayesianModel

model = BayesianModel([('A', 'B'), ('B', 'C')])
model.add_cpds(P_A, P_B_I_A, P_C_I_B)
model.check_model()

# True
```

graphviz와 pydot 패키지를 사용하여 시각화 할 수도 있다.

```python
from IPython.core.display import Image
from networkx.drawing.nx_pydot import to_pydot

d = to_pydot(model)
d.set_dpi(300)
d.set_margin(0.2)
d.set_rankdir("LR")
Image(d.create_png(), width=600)
```

![](https://user-images.githubusercontent.com/17154958/51101239-3000fc80-181d-11e9-9c04-225b8882362d.png)

이렇게 만들어진 모형으로부터 여러가지 추론(inference)을 할 수 있다. 예를 들어 전체 결합확률분포 함수를 찾고 그 함수로부터 A, B, C의 marginal 확률분포를 계산하면 A, B, C의 값으로 어떤 값이 가장 확률이 높은지 알 수 있다. 분석 결과를 보면 시험 성적이 좋을 확률은 35.9%이다. 추론에 사용된  `VariableElimination` 클래스의 사용법에 대해서는 곧 학습한다.

```python
from pgmpy.inference import VariableElimination
inference = VariableElimination(model)
result = inference.query(variables=["C"])
print(result["C"])
```

![](https://user-images.githubusercontent.com/17154958/51101318-84a47780-181d-11e9-92da-b71f6c1adc55.png)



### 베이지안 네트워크의 결합확률분포

베이지안 네트워크를 만들려면 조사 대상이 되는 확률변수를 노드(node)로 생성하고 인과관계가 있는 노드를 방향성 간선(directed edge)로 연결하는 것이다. 이렇게 베이지안 네트워크가 만들어지면 이 확률변수들의 결합확률분포는 다음처럼 주어진다.
$$
P(X_1, \cdots, X_N) = \prod_{i=1}^N P(X_i \mid Pa(X_i))
$$
이 식에서 $Pa(X_i)$ 는 $X_i$ 의 부모 노드이다.

```python
g1 = nx.DiGraph()
g1.add_edge("X1", "X3")
g1.add_edge("X1", "X4")
g1.add_edge("X3", "X4")
g1.add_edge("X2", "X4")
g1.add_edge("X2", "X7")
g1.add_edge("X4", "X5")
g1.add_edge("X4", "X6")

d1 = to_pydot(g1)
d1.set_dpi(300)
d1.set_margin(0.2)
d1.set_rankdir("LR")
Image(d1.create_png(), width=800)
```

![](https://user-images.githubusercontent.com/17154958/51101438-fd0b3880-181d-11e9-8bfb-05dd34c1ba7a.png)

예를 들어 $X_1, \cdots, X_6$ 의 관계가 앞 그래프와 같다면 결합확률분포는 다음과 같다.
$$
P(X_1, X_2, X_3, X_4, X_5, X_6, X_7) = \\
P(X_1)P(X_2)P(X_3 \mid X_1)P(X_4 \mid X_2,X_3)p(X_5 \mid X_4)P(X_6 \mid X_4)P(X_7 \mid X_2)
$$


### 조건부 독립

베이지안 네트워크를 만들 때 중요한 것은 확률변수간의 조건부 독립 관계가 그래프에 나타나고 있어야 한다는 점이다. 조건부 독립(conditional independence)은 일반적인 독립과 달리 조건이 되는 확률변수가 존재해야 한다. 일반적으로는 확률변수 A, B가 독립인 정의는 다음과 같다.
$$
P(A, B) = P(A)P(B)
$$
조건부 독립은 조건이 되는 C라는 확률변수에 대한 조건부 결합확률분포에 대해 다음이 만족되어야 한다.
$$
P(A, B \mid C) = P(A \mid C) P(B \mid C)
$$
즉, C에 대한 조건부 결합확률분포가 조건부 확률분포의 곱으로 나타난다.

기호로는 다음과 같이 표기한다.
$$
A \text{⫫} B \;\vert\; C
$$
같은 방식으로 무조건부 독립은 다음과 같이 표기하기도 한다.
$$
A \text{⫫} B \; \vert\; \emptyset
$$
A, B가 C에 대해 조건부 독립이면 다음도 만족한다.
$$
P(A \mid B, C) = P(A \mid C)\\
P(B \mid A, C) = P(B \mid C)
$$
주의할 점은 조건부 독립과 (무조건부) 독립은 관계가 없다는 점이다. 즉, 두 확률변수가 독립이라고 항상 조건부 독립이 되는 것도 아니고 조건부 독립이라고 꼭 독립이 되는 것도 아니다.
$$
P(A, B) = P(A)P(B) \;\; \bcancel{\implies} \;\; P(A,B \mid C) = P(A \mid C)P(B \mid C) \\
P(A,B \mid C) = P(A \mid C)P(B \mid C) \;\; \bcancel{\implies} \;\;  P(A, B) = P(A)P(B)
$$


### 방향성 분리

