# 신경망 기초 이론

신경망(neural network) 모형은 기저 함수(basis function)의 형태를 모수(parameter) 값으로 변화시킬 수 있는 적응형 기저 함수 모형(adaptive basis function model)이며 구조적으로는 복수의 퍼셉트론을 쌓아놓은 형태이므로 MLP(multi-layer perceptron)로도 불린다.



### 퍼셉트론의 복습

다음 그림과 같이 독립 변수 벡터가 3차원인 간단한 퍼셉트론 모형을 살펴보자.

![](https://user-images.githubusercontent.com/17154958/51251024-9fbce600-19db-11e9-91d6-40f6be64a509.png)

- 입력 $x$

$$
x = \begin{bmatrix}X_1 \\ X_2 \\ X_3 \end{bmatrix}
$$

- 가중치 $w$

$$
w = \begin{bmatrix}w_1 \\ w_2 \\ w_3 \end{bmatrix}
$$

- 바이어스(y 절편) $b​$

$$
b
$$

- 활성화 함수 입력값 $a$

$$
a = \sum_{i=1}^3 w_ix_i + b = W^Tx + b
$$

- 활성화 함수(activation function) $h$ 와 활성화 함수 출력값 $z$

$$
z = h(a) = h(w^Tx + b)
$$

- 최종 출력 $\hat{y}$

$$
\hat{y} = z
$$



### 시그모이드 활성화 함수

일반적으로 활성화 함수 $h$ 로는 위와 앙래가 막혀있는(bounded) 시그모이드 함수 $\sigma$ 를 사용하는데 가장 많이 사용하는 함수는 다음과 같은 로지스틱 함수이다.
$$
h(a) = \sigma(a) = \dfrac{1}{1 + e^{-a}}
$$
시그모이드 함수의 미분은 다음처럼 쉽게 계산할 수 있다.
$$
\dfrac{d\sigma(a)}{da} = \sigma(a)(1 - \sigma(a)) = \sigma(a)\sigma(-a)
$$
활성화 함수로 로지스틱 함수를 사용할 때는 $z = h(a)$ 값으로부터 최종 클래스 결정값 $\hat{y}$ 를 다음 식으로 구한다.
$$
\hat{y} = \text{sign} \left( z - \dfrac{1}{2} \right) = \text{round}(z)
$$


### 비선형 기저 함수

이런 퍼셉트론에서 $x$ 대신 기저함수 $\phi(x)$ 를 사용하면 XOR 문제 등의 비선형 문제를 해결할 수 있다. 그러나 고정된 기저 함수를 사용해야 하므로 문제에 맞는 기저 함수를 찾아야 한다는 단점이 있다. 따라서 $J$ 개의 많은 기저 함수를 사용하는 것이 보통이다.
$$
z = h \left( \sum_{j=1}^J w_j\phi_j(x) + b \right) = h(w^T\phi(x) + b)
$$

### 

### 하이퍼 파라미터에 의해 모양이 바뀌는 비선형 기저 함수

만약 기저 함수 $\phi(x)$ 의 형태를 추가적인 모수 $\theta$ 를 사용하여 조절할 수 있다면 즉, 기저함수 $\phi(x;\theta)$ 를 사용하면 $\theta$ 값을 바꾸는 것만으로 다양한 모양의 기저 함수를 시도할 수 있다.
$$
z = h(w^T \phi(x; \theta) + b)
$$
신경망 즉, MLP(Multi-Layer Perceptron)은 퍼셉트론이 사용하고 있는 로지스틱 시그모이드 함수를 기저 함수로 사용하는 모형이다. 기저 함수의 형태는 하이퍼 파라미터 $w^{(1)}, b^{(1)}$ 의 값에 따라서 바꿀 수 있다.
$$
\phi_j(x; \theta_j) = \phi_j(x; w_j^{(1)}, b_j^{(1)}) = h(w_j^{(1)}x + b_j^{(1)})
$$
최종 출력값은 다음과 같다.
$$
z = h \left( \sum_{j=1}^M w_jh(w_j^{(1)}x + b_j^{(1)}) + b \right)
$$
이 식에서 각각의 계수의 역할은 다음과 같다.

- $w^{(1)}, b^{(1)}$ : 기저 함수의 모양 조절
- $w, b$ : 결정 함수, 즉 경계면 직선의 모양 조절



### Universal Approximation Theorem

Universal Approximation Theorem에 따르면 위와 같은 적응형 기저 함수 $h(w_j^{(1)}x + b_j^{(1)})$ 를 충분히 많이 사용하면 어떠한 형태의 함수와도 유사한 형태의 함수 $z(x)$ 를 만들 수 있다.



### 다계층 퍼셉트론

신경망은 퍼셉트론을 여러개 연결한 것으로 다계층 퍼셉트론(MLP: Multi-Layer Perceptrons)이라고도 한다. 신경망에 속한 퍼셉트론은 뉴론(neuron) 또는 노드(node)라고 불린다.

각 계층(layer)은 다음 계층에 대해 적응형 기저 함수의 역할을 한다. 최초의 계층은 입력 계층(input layer), 마지막 계층은 출력 계층(output layer)이라고 하며 중간은 은닉 계층(hidden layer)라고 한다.

![](https://user-images.githubusercontent.com/17154958/51311766-fafae100-1a8c-11e9-936a-1ac4cdce332f.png)

MLP의 또다른 특징은 출력 계층에 복수개의 출력 뉴련을 가지고 각 뉴런값으로 출력 클래스의 조건부 확률을 반환하도록 설계하여 멀티 클래스 문제를 해결할 수도 있다는 점이다.

다음은 필기 숫자에 대한 영상 정보를 입력 받아 숫자 0 ~ 9 까지의 조건부 확률을 출력하는 MLP의 예이다. 입력 영상이 28 X 28 해상도를 가진다면 입력 계층의 뉴런 수는 28 X 28 = 784 개가 된다. 출력은 숫자 0 ~ 9 까지의 조건부 확률을 출력하는 10 개의 뉴런을 가진다.

그림의 모형은 15개의 뉴런을 가지는 1개의 은닉 계층을 가진다.

![](https://user-images.githubusercontent.com/17154958/51313401-cf79f580-1a90-11e9-8404-53d8ced5b48d.png)



### 신경망 가중치 표기법

신경망의 가중치는 $w_{j,i}^{(l)}$ 과 같이 표기한다. 이 가중치는 $l - 1$ 번째 계층의 $i$ 번째 뉴련과 $l$ 번째 계층의 $j$ 번째 뉴런을 연결하는 가중치를 뜻한다. 첨자의 순서에 주의한다.

![](https://user-images.githubusercontent.com/17154958/51313500-0b14bf80-1a91-11e9-9ae7-e26c04269f8a.png)

이러한 방식을 사용하면 $l-1$ 번째 레이어의 출력값 벡터 $z^{(l-1)}$ 과 $l$ 번째 레이어의 입력값 벡터 $a^{(l)}$ 간에는 다음과 같은 식이 성립한다.
$$
a^{(l)} = W^{(l)}z^{(l-1)} + b^{(l)}
$$
이 식에서 $W^{(l)}$ 는 $l-1$ 번째 레이어와 $l$ 번째 레이어의 사이를 연결하는 가중치 값 행렬이고 $b^{(l)}$ 은 $l$ 번째 레이어의 뉴런의 바이어스 값 벡터이다.



### 순방향 전파





### 역전파

단순하게 수치적으로 미분을 계산한다면 모든 가중치에 대해서 개별적으로 미분을 계산해야 한다. 그러나 역전파(back propagation) 방법을 사용하면 모든 가중치에 대한 미분값을 한번에 계산할 수 있다.

역전파 방법을 수식으로 표현하면 다음과 같다.

우선 $\delta$ 를 뒤에서 앞으로 전파한다. $\delta$ 는 다음과 같이 정의되는 값이다.
$$
\delta_j^{(l)} = \dfrac{\partial C}{\partial a_j^{(l)}} \\ 
\delta_j^{(l-1)} = h'(a_j^{(l-1)}) \sum_{i=1}^{N_{(l)}} w_{ij}^{(l)}\delta_i^{(l)}
$$
위 식에서 $N_{(l)}$ 는 $l$ 번째 레이어의 노드의 갯수이다.

이 식을 벡터-행렬 식으로 쓰면 다음과 같다.
$$
\delta^{(l-1)} = h'(a^{(l-1)}) \odot ({W^T}^{(l)} \delta^{(l)})
$$
여기에서 $\odot$  연산 기호는 Hadamard Product, Schur product, 혹은 element-wise product 라고 불리는 연산으로 정의는 다음과 같다. 즉 Numpy의 일반적인 배열 곱과 같다.
$$
x \odot y = 
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right) \odot
\left(\begin{array}{c} y_1 \\ y_2 \\ y_3 \end{array}\right) 
= \left(\begin{array}{c} x_1 y_1 \\ x_2 y_2 \\ x_3 y_3 \end{array}\right)
$$
