# 모형 결합

- aggregation 취합
- boosting 부스팅

hard 동등하게 하나의 투표권을 가진다

soft 각 모델에서 나온 확률을 더한다

가정: 개별 모델들은 독립적으로 판단한다.

### 모형 결합을 사용한 성능 향상

$$
\sum_{k>\frac{N}{2}}^N \binom N k p^k (1-p)^{N-k}
$$



### 배깅

앞서 모형 결합에서 사용하는 독립적인 모형의 수가 많을 수록 성능 향상이 일어날 가능성이 높다는 것을 알았다. 각각 다른 확률 모형을 사용하는데에는 한계가 있으므로 보통은 배깅 방법을 사용하여 같은 확률 모형을 쓰지만 서로 다른 결과를 출력하는 다수의 모형을 만든다.

배깅(bagging)은 동일한 모형과 모형 모수를 사용하는 대신 부트스트래핑(bootstrapping)과 유사하게 트레이닝 데이터를 랜덤하게 선택해서 다수결 모형을 적용한다.

트레이닝 데이터를 선택하는 방법에 따라 다음과 같이 부르기도 한다.

- 같은 데이터 샘플을 중복사용(replacement)하지 않으면: Pasting
- 같은 데이터 샘플을 중복사용(replacement)하면 Bagging
- 데이터가 아니라 다차원 독립 변수 중 일부 차원을 선택하는 경우에는: Random Subspaces
- 데이터 샘플과 독립 변수 차원 모두 일부만 랜덤하게 사용하면: Random Patches

성능 평가시에는 트레이닝용으로 선택한 데이터가 아닌 다른 데이터를 사용할 수도 있다. 이런 데이터를 OOB(out-of-bag) 데이터라고 한다.

배깅은 데이터를 나눠가진다. 한 모델에는 데이터의 일부가 들어간다. 데이터 일부에 outlier가 속해있을 가능성은 낮다.



### 랜덤 포레스트

랜덤 포레스트(Random Forest)는 의사 결정 나무를 개별 모형으로 사용하는 모형 결합 방법을 말한다.

Decision Tree : greedy

Random Forest : non-greedy

extreme random forest

ExtraTreesClassifier 모든 선수들의 출전 기회가 동등하다.

