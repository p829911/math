# 엔트로피

### 엔트로피의 정의

$Y = 0$ 또는 $Y=1$인 두 가지 값을 가지는 확률 분포가 다음과 같이 세 종류가 있다고 하자

확률 분포들이 가지는 확신의 정도를 수치로 표현하는 것을 엔트로피(entropy)라고 한다.

엔트로피는 수학적으로 다음과 같이 정의한다

확률변수 $Y$가 베르누이나 카테고리 분포와 같은 이산 확률변수이면
$$
H[Y] = -\sum_{k=1}^K P(y_k) log_2 P(y_k)
$$

이 식에서 $K$는 $X$가 가질 수 있는 클래스의 수이고 $P(y)$는 확률질량함수이다.

확률변수 $Y$가 연속 확률변수이면
$$
H[Y] = - \int_{-\infin}^\infin p(y) log_2p(y)\;dy
$$
이 식에서 $P(y)$는 확률밀도함수이다.

로그의 밑(base)이 2로 정의된 것은 정보통신과 관련을 가지는 역사적인 이유 때문이다.

위에서 예를 든 $Y_1$, $Y_2$, $Y_3$에 대해 엔트로피를 구하면 다음과 같다.
$$
\begin{eqnarray}
H[Y_1] &=& -\frac{1}{2}log_2\frac{1}{2} - \frac{1}{2}log_2\frac{1}{2} &=& 1 \\\\
H[Y_2] &=& -\frac{8}{10}log_2\frac{8}{10} - \frac{2}{10}log_2\frac{2}{10} &=& 0.72 \\\\
H[Y_3] &=& -1log_21 - 0log_20 &=& 0
\end{eqnarray}
$$
엔트로피 계산에서 $p(y) = 0$인 경우에는 다음과 같은 극한값을 사용한다. 이 값은 로피탈의 정리에서 구할 수 있다. 

### 엔트로피의 성질

확률변수가 결정론적이면 확률분포에서 특정한 하나의 값이 나올 확률이 1이다. 이 때 엔트로피는 0이 되고 이 값은 엔트로피가 가질 수 있는 최솟값이다.

반대로 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산 확률변수가 가질 수 있는 클래스가 $2^K$개이고 이산 확률변수가 가질 수 있는 엔트로피의 최대값은 각 클래스가 모두 같은 확률을 가지는 때이다.

이 때 엔트로피의 값은
$$
H = -\frac{2^K}{2^K}log_2\frac{1}{2^K} = K
$$

### 엔트로피와 정보량

엔트로피는 확률변수가 담을 수 있는 정보의 양을 의미한다고 볼 수도 있다.  확률변수가 담을 수 있는 정보량(information)이란 확률변수의 표본값을 관측해서 얻을 수 있는 추가적인 정보의 종류를 말한다.

엔트로피가 0이면 확률변수는 결정론적이므로 확률 변수의 표본값은 항상 같다. 따라서 확률 변수의 표본값을 관측한다고 해도 우리가 얻을 수 있는 추가 정보는 없다.

반대로 엔트로피가 크다면 확률변수의 표본값이 가질 수 있는 경우의 수가 증가하므로 표본값을 실제로 관측하기 전까지는 아는 것이 거의 없다. 반대로 말하면 확률변수의 표본값이 우리에게 가져다 줄 수 있는 정보의 양이 많다.



### 엔트로피와 무손실 인코딩

엔트로피는 원래 통신 분야에서 데이터가 가지고 있는 정보량을 계산하기 위해 고안되었다. 예를 들어 4개의 알파벳 A, B, C, D로 씌어진 문서가 있다고 하자. 이 문서를 0과 1로 이루어진 이진수로 변환할 때 일반적인 경우라면 다음과 같이 인코딩을 할 것이다.

- A = "00"
- B = "01"
- C = "10"
- D = "11"

이렇게 인코딩을 하면 1,000 글자로 이루어진 문서는 이진수 2,000개가 된다.

만약 문서에서 각 알파벳이 나올 확률이 동일하지 않고 다음과 같다고 가정하다.
$$
\Big\{ \dfrac{1}{2}, \dfrac{1}{4}, \dfrac{1}{8}, \dfrac{1}{8} \Big\}
$$
이 때는 다음과 같이 가변길이 인코딩(variable length encoding)을 하면 인코딩된 이진수의 수를 줄일 수 있다.

- A = "0"
- B = "10"
- C = "110"
- D = "111"

인코딩된 이진수의 숫자는 다음 계산에서 약 1,750개가 됨을 알 수 있다.
$$
\left(1000 \times \frac{1}{2}\right) \cdot 1 + 
\left(1000 \times \frac{1}{4}\right) \cdot 2 + 
\left(1000 \times \frac{1}{8}\right) \cdot 3 + 
\left(1000 \times \frac{1}{8}\right) \cdot 3 
= 1750
$$
1.75는 알파벳 한 글자를 인코딩하는데 필요한 평균 비트(bit)수이며 확률변수의 엔트로피 값과 같다.
$$
H = -\frac{1}{2}log_2\frac{1}{2}-\frac{1}{4}log_2\frac{1}{4}-\frac{2}{8}log_2\frac{1}{8} = 1.75 
$$
